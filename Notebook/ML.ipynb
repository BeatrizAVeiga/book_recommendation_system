{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt                                                           \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import bertopic\n",
    "import re\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize, sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataframe containing the information in regards to authors\n",
    "df_authors = pd.read_json(r\"C:\\Users\\biave\\Desktop\\goodreads_book_authors.json\", lines=True)\n",
    "\n",
    "# Loading the dataframe containing the books related to mystery, thriller and crime\n",
    "df_books = pd.read_json(r\"C:\\Users\\biave\\Desktop\\goodreads_books_history_biography.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean & Transform Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Books & Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books['authors'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_ids(authors):\n",
    "    \"\"\"Extracting the author IDs\"\"\"\n",
    "    author_ids = []  \n",
    "    if isinstance(authors, list):  \n",
    "        for author in authors:\n",
    "            if \"author_id\" in author:\n",
    "                author_ids.append(author[\"author_id\"]) \n",
    "    return author_ids  \n",
    "\n",
    "\n",
    "df_books[\"author_ids\"] = df_books[\"authors\"].apply(extract_author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_author_id(author_ids):\n",
    "    \"\"\"Getting the first author ID\"\"\"\n",
    "    return author_ids[0] if isinstance(author_ids, list) else None\n",
    "\n",
    "df_books['first_author_id'] = df_books['author_ids'].apply(get_first_author_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unecessary columns\n",
    "df_books.drop(columns=[\"text_reviews_count\", \"series\", \"country_code\", \"popular_shelves\", \"asin\", \n",
    "                       \"kindle_asin\", \"edition_information\", \"url\", \"work_id\", \"link\", \"publication_day\", \n",
    "                       \"publication_month\", \"title_without_series\", \"publisher\", \"isbn13\",\n",
    "                       \"author_ids\", \"authors\",\"similar_books\", \"image_url\"], \n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting both columns from both DFs to strings to be able to merge them\n",
    "df_books['first_author_id'] = df_books['first_author_id'].astype(str)\n",
    "df_authors['author_id'] = df_authors['author_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging both DFs to get the authors' names\n",
    "df_books = pd.merge(df_books, df_authors, left_on='first_author_id', right_on='author_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "df_books.rename(columns={'average_rating_x': 'avg_rating_books', 'ratings_count_x': 'rating_count_books', 'average_rating_y': 'avg_rating_authors', 'ratings_count_y': 'rating_count_authors'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the most convenient column order\n",
    "desided_order = ['isbn', \"book_id\", \"title\", \"description\", \"publication_year\", \"avg_rating_books\", \"rating_count_books\", \"name\", \"first_author_id\", \"avg_rating_authors\", \"format\", \"num_pages\", \"language_code\"]\n",
    "\n",
    "df_books = df_books[desided_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_books['language_code'].unique())  \n",
    "english_codes = {'eng', 'en-GB', 'en-US', 'en-CA', 'en', 'aus'}\n",
    "\n",
    "df_books = df_books[df_books['language_code'].isin(english_codes)].reset_index(drop=True)\n",
    "df_books.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_books['language_code'].unique())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books['publication_year'] = df_books['publication_year'].astype(str).str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for rows with empty strings \n",
    "(df_books == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting instances in which the rows contain empty strings\n",
    "df_books = df_books[(df_books != '').all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.to_csv('books.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Books Dataframe:\")\n",
    "display(df_books.head(5))\n",
    "print(\"Authors Dataframe:\")\n",
    "display(df_authors.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_exploration(df):\n",
    "    \"\"\"Data Exploration\"\"\"\n",
    "    display(df.head())\n",
    "    print(\"Data Types:\")\n",
    "    display(df.dtypes)\n",
    "    print(\"Data Overview:\")\n",
    "    display(df.info())\n",
    "    print(\"Summary Statistics:\")\n",
    "    display(df.describe())\n",
    "    print(\"Missing Values:\")\n",
    "    display(df.isnull().sum())\n",
    "    print(\"Rows with empty Strings:\")\n",
    "    print((df_books == '').sum())\n",
    "    print(\"Missing Values %:\")\n",
    "    missing_percentage = df.isnull().mean() * 100\n",
    "    display(missing_percentage)\n",
    "    print(\"Duplicates:\")\n",
    "    display(df.duplicated().sum())\n",
    "    print(\"Duplicates %:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    percentage = (duplicates / len(df)) * 100\n",
    "    print(f'The dataset contains {duplicates} duplicate rows, making up {percentage}% of the total number of records.')\n",
    "\n",
    "data_exploration(df_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv(r\"C:\\Users\\biave\\Desktop\\books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying SBERT + BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the necessary stopwords, tokenizer, lemmatizer and SBERT Model\n",
    "tokenizer_nltk = WordPunctTokenizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sbert_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper() \n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans and preprocesses text.\"\"\"\n",
    "    text = re.sub('<[^>]*>', '', text) \n",
    "    text = text.lower() \n",
    "    tokens = tokenizer_nltk.tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords_set]\n",
    "    tokens = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbert_embedding(text):\n",
    "    \"\"\"Generates SBERT embeddings for the given text.\"\"\"\n",
    "    return sbert_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing descriptions\n",
    "df_books = df_books.dropna(subset=['clean_description'])\n",
    "df_books = df_books.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.reset_index(inplace=True)\n",
    "df_books.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings from description only\n",
    "embeddings = np.array([get_sbert_embedding(desc) for desc in df_books['clean_description']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing and fitting the BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=5, n_components=5, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=sbert_model,  \n",
    "    umap_model=umap_model,        \n",
    "    hdbscan_model=hdbscan_model,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _ = topic_model.fit_transform(df_books['clean_description'], embeddings)\n",
    "df_books['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.fit(df_books['clean_description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books['topic'] = topic_model.transform(df_books['clean_description'].tolist())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.to_csv('books_w_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "for n_neighbors in [5, 15, 50]:\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=2)\n",
    "    reduced_data = umap_model.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], label=f\"n_neighbors={n_neighbors}\")\n",
    "    plt.title(f\"UMAP with n_neighbors={n_neighbors}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_bertopic(user_description, df_books, top_n=10, min_rating=4.0):\n",
    "    # Get the user's input embedding\n",
    "    user_input_embedding = get_sbert_embedding(user_description).reshape(1, -1)\n",
    "\n",
    "    # Calculate similarities between user input and book descriptions\n",
    "    similarities = cosine_similarity(user_input_embedding, embeddings).flatten()\n",
    "\n",
    "    # Find the most similar topics to the user's input (using cosine similarity)\n",
    "    user_topic = topic_model.transform([user_description])[0][0] \n",
    "\n",
    "    # Filter the books that belong to the same topic\n",
    "    recommendations = df_books[df_books['topic'] == user_topic]\n",
    "\n",
    "    # Get the indices of the recommendations\n",
    "    recommendation_indices = recommendations.index\n",
    "\n",
    "    # Filter similarities to only include recommendations\n",
    "    similarities_filtered = similarities[recommendation_indices]\n",
    "\n",
    "    # Add the similarity score to the recommendations\n",
    "    recommendations['similarity'] = similarities_filtered\n",
    "\n",
    "    # Filter by rating\n",
    "    recommendations = recommendations[recommendations['avg_rating_books'] >= min_rating]\n",
    "\n",
    "    # Sort by similarity and rating\n",
    "    recommendations = recommendations.sort_values(by=['similarity', 'avg_rating_books'], ascending=[False, False])\n",
    "\n",
    "    # Return top N books\n",
    "    return recommendations[['title', 'avg_rating_books', 'name', 'description', 'similarity', 'topic']].head(top_n)\n",
    "\n",
    "# Example Usage:\n",
    "user_description = \"Books about LGBTQ+ Activism\"\n",
    "recommended_books = recommend_books_bertopic(user_description, df_books, top_n=5)\n",
    "print(recommended_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Bert - Just to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary stopwords and tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "tokenizer_nltk = WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper() \n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans and preprocesses text.\"\"\"\n",
    "    text = re.sub('<[^>]*>', '', text)  # Remove HTML tags\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = tokenizer_nltk.tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords_set]\n",
    "    tokens = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books['clean_description'] = df_books['description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    \"\"\"Generates BERT embeddings for the given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing descriptions\n",
    "df_books = df_books.dropna(subset=['clean_description'])\n",
    "df_books = df_books.sample(frac=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.to_csv('reduced_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.reset_index(inplace=True)\n",
    "df_books.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings from description only\n",
    "embeddings = np.array([get_bert_embedding(desc) for desc in df_books['clean_description']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_bert(user_description, df_books, top_n=10, preferred_format=None, min_rating=0):\n",
    "    # Finding the book's description from the user chosen title\n",
    "    user_input_embedding = get_bert_embedding(user_description).reshape(1, -1)\n",
    "    \n",
    "    # Finding the similarities\n",
    "    similarities = cosine_similarity(user_input_embedding, embeddings).flatten()\n",
    "    \n",
    "    # Creating a DF with the similatities\n",
    "    recommendations = df_books.copy()\n",
    "    recommendations['similarity'] = similarities\n",
    "    \n",
    "    # Filtering by format\n",
    "    if preferred_format:\n",
    "        recommendations = recommendations[recommendations['format'] == preferred_format]\n",
    "    \n",
    "    recommendations = recommendations[recommendations['avg_rating_books'] >= min_rating]\n",
    "    \n",
    "    # Sort by similarity and rating (higher ratings first)\n",
    "    recommendations = recommendations.sort_values(by=['similarity', 'avg_rating_books'], ascending=[False, False])\n",
    "\n",
    "    # Return the top N recommended books\n",
    "    return recommendations[['title', 'avg_rating_books', 'name', 'description', 'similarity', 'format']].head(top_n)\n",
    "\n",
    "# Example: User inputs a book title\n",
    "user_description = \"Biographies of writers, artists, and philosophers.\"\n",
    "\n",
    "recommended_books = recommend_books_bert(user_description, df_books, top_n=5, preferred_format=\"Paperback\", min_rating=4.0)\n",
    "\n",
    "# Display recommendations\n",
    "print(recommended_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans and preprocesses text with stemming.\"\"\"\n",
    "    text = re.sub('<[^>]*>', '', text)  \n",
    "    text = text.lower() \n",
    "    tokens = word_tokenize(text)  \n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords_set]  \n",
    "    tokens = [stemmer.stem(word) for word in tokens]  \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_bert(user_input_title, df_books, top_n=10, preferred_format=None, min_rating=0):\n",
    "    # Finding the book's description from the user chosen title\n",
    "    user_input_embedding = get_bert_embedding(user_description).reshape(1, -1)\n",
    "    \n",
    "    # Finding the similarities\n",
    "    similarities = cosine_similarity(user_input_embedding, embeddings).flatten()\n",
    "    \n",
    "    # Creating a DF with the similatities\n",
    "    recommendations = df_books.copy()\n",
    "    recommendations['similarity'] = similarities\n",
    "    \n",
    "    # Filtering by format\n",
    "    if preferred_format:\n",
    "        recommendations = recommendations[recommendations['format'] == preferred_format]\n",
    "    \n",
    "    recommendations = recommendations[recommendations['avg_rating_books'] >= min_rating]\n",
    "    \n",
    "    # Sort by similarity and rating (higher ratings first)\n",
    "    recommendations = recommendations.sort_values(by=['similarity', 'avg_rating_books'], ascending=[False, False])\n",
    "\n",
    "    # Return the top N recommended books\n",
    "    return recommendations[['title', 'avg_rating_books', 'name', 'description', 'similarity', 'format']].head(top_n)\n",
    "\n",
    "# Example: User inputs a book title\n",
    "user_description = \"Biographies of writers, artists, and philosophers.\"\n",
    "\n",
    "recommended_books = recommend_books_sbert(user_input_title, df_books, top_n=5, preferred_format=\"Paperback\", min_rating=4.0)\n",
    "\n",
    "# Display recommendations\n",
    "print(recommended_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Initialize KMeans model\n",
    "kmeans_model = KMeans(random_state=42)\n",
    "\n",
    "visualizer = KElbowVisualizer(kmeans_model, k=(2, 20)) \n",
    "\n",
    "visualizer.fit(embeddings)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KMeans clustering\n",
    "# Change the number of clusters based on the Elbow vizualizer above\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=labels, palette='PuRd', s=50, edgecolor='k')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='lightgreen', marker='X', s=200, label='Centroids')\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"Cluster Visualization using PCA\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the KMeans model\n",
    "with open(\"kmeans_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "# Save the PCA model\n",
    "with open(\"pca_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "# Save BERT embeddings\n",
    "with open(\"sbert_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "# Save BERTopic model\n",
    "with open('bertopic_model.pkl', 'wb') as f:\n",
    "    pickle.dump(topic_model, f)\n",
    "\n",
    "# Save SentenceTransformer (SBERT) model\n",
    "sbert_model.save('sbert_model') \n",
    "\n",
    "with open('umap_model.pkl', 'wb') as f:\n",
    "    pickle.dump(umap_model, f)\n",
    "\n",
    "with open('hdbscan_model.pkl', 'wb') as f:\n",
    "    pickle.dump(hdbscan_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
